{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52c17e17",
   "metadata": {},
   "source": [
    "<b> Name : </b> Anne Sai Venkata Naga Saketh <br>\n",
    "<b> USC Email : </b> annes@usc.edu <br>\n",
    "<b> USC ID : </b> 3725520208 <br>\n",
    "<b> CSCI 544 - Applied Natural Language Processing </b> <br>\n",
    "<b> Homework 4 </b> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49e3f79",
   "metadata": {},
   "source": [
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa7cc944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing sys module for system-specific parameters and functions\n",
    "import sys\n",
    "# Importing json module for JSON (JavaScript Object Notation) encoding and decoding\n",
    "import json\n",
    "# Importing PyTorch, a popular deep learning framework\n",
    "import torch\n",
    "# Importing neural network module from PyTorch\n",
    "import torch.nn as nn\n",
    "# Importing optimization module from PyTorch\n",
    "import torch.optim as optim\n",
    "# Importing performance metrics from scikit-learn\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "# Importing numpy, a fundamental package for scientific computing in Python\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42baadee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the Vocabulary is : 23624\n",
      "The size of the Tags Frequency is : 9\n",
      "The size of the lines is : 219553\n"
     ]
    }
   ],
   "source": [
    "# Path to the input data file\n",
    "input_path = \"data/train\"  \n",
    "\n",
    "# List to store lines of data\n",
    "l_list = []  \n",
    "\n",
    "# Dictionary to store vocabulary word frequency\n",
    "v = {}  \n",
    "\n",
    "# Dictionary to store tag frequency\n",
    "t_f = {} \n",
    "\n",
    "# Counter for sentence endings\n",
    "s_c = 0  \n",
    "\n",
    "# Read all lines from the file and Iterate over each line\n",
    "with open(input_path) as file:\n",
    "    input_lines = file.readlines()  \n",
    "    for line in input_lines:  \n",
    "        words = line.split(\" \")\n",
    "        \n",
    "        # List to store current line data\n",
    "        current_line = []  \n",
    "        \n",
    "        if len(words) == 1:\n",
    "            # Append an empty string to current line\n",
    "            current_line.append(\" \")\n",
    "            \n",
    "            # Append current line to the list\n",
    "            l_list.append(current_line)  \n",
    "            continue\n",
    "        # Get the index, current word, tag from the line and append the current word to the line and the tag to the line\n",
    "        else:\n",
    "            index = words[0].strip()\n",
    "            current_w = words[1].strip()\n",
    "            tag = words[2].strip()\n",
    "            current_line.append(index)\n",
    "            current_line.append(current_w)\n",
    "            current_line.append(tag)\n",
    "            \n",
    "            if current_w == '.':\n",
    "                # Increment the sentence ending counter\n",
    "                s_c += 1  \n",
    "            \n",
    "            # If current word is already in vocabulary\n",
    "            if current_w in v:  \n",
    "                # Increment its frequency\n",
    "                v[current_w] += 1  \n",
    "            # If current word is not in vocabulary\n",
    "            else:  \n",
    "                # Add it to vocabulary with frequency 1\n",
    "                v.update({current_w: 1})  \n",
    "\n",
    "            # If tag is already in tag frequency dictionary\n",
    "            if tag in t_f:  \n",
    "                # Increment its frequency\n",
    "                t_f[tag] += 1  \n",
    "            # If tag is not in tag frequency dictionary\n",
    "            else:  \n",
    "                # Add it to tag frequency dictionary with frequency 1\n",
    "                t_f.update({tag: 1}) \n",
    "                \n",
    "        # Append current line to the list\n",
    "        l_list.append(current_line)  \n",
    "\n",
    "# Print sizes of vocabulary, tag frequency, and lines\n",
    "print(\"The size of the Vocabulary is :\", len(v))\n",
    "print(\"The size of the Tags Frequency is :\", len(t_f))\n",
    "print(\"The size of the lines is :\", len(l_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2990d822",
   "metadata": {},
   "source": [
    "### Task:1 -- Simple Bi-Directional LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd72057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store sentences\n",
    "sents = []  \n",
    "# List to store tags\n",
    "t = []  \n",
    "# List to store current sentence words\n",
    "present_s = []  \n",
    "# List to store current sentence tags\n",
    "present_t = []  \n",
    "\n",
    "# Iterate over each line in l_list\n",
    "for l in l_list:\n",
    "    # If the line contains only one element (empty line indicating end of sentence)\n",
    "    if len(l) == 1:  \n",
    "        t.append(present_t)  \n",
    "        # Reset present_t to empty list for the next sentence\n",
    "        present_t = []  \n",
    "        sents.append(present_s)  \n",
    "        # Reset present_s to empty list for the next sentence\n",
    "        present_s = []  \n",
    "    # If the line contains more than one element\n",
    "    else:  \n",
    "        present_t.append(l[2])  \n",
    "        present_s.append(l[1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d43b528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to map words to indices\n",
    "w2i = {}  \n",
    "# Dictionary to map tags to indices\n",
    "t2i = {}  \n",
    "# Initial index value\n",
    "index_value = 0  \n",
    "\n",
    "# Iterate over each unique tag in tag frequency dictionary\n",
    "for tag in t_f:\n",
    "    # If the tag already exists in the tag-to-index dictionary\n",
    "    if tag in t2i:  \n",
    "        continue  \n",
    "    # If the tag is new\n",
    "    else:  \n",
    "        # Add tag to tag-to-index dictionary with current index value\n",
    "        t2i.update({tag: index_value})  \n",
    "        # Increment index value for the next tag\n",
    "        index_value += 1  \n",
    "\n",
    "# Iterate over each sentence in the list of sentences\n",
    "for s in sents:\n",
    "    # Iterate over each word in the sentence\n",
    "    for word in s:  \n",
    "        # If the word already exists in the word-to-index dictionary\n",
    "        if word in w2i:  \n",
    "            continue  \n",
    "        # If the word is new\n",
    "        else:  \n",
    "            # Add word to word-to-index dictionary with current index value\n",
    "            w2i.update({word: len(w2i)})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "671f39d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store tensor representations of sentences\n",
    "s_tensors = []\n",
    "\n",
    "# Iterate over each sentence in the list of sentences\n",
    "for s in sents:\n",
    "    # Convert words to indices using word-to-index dictionary\n",
    "    sent_ind = [w2i[word] for word in s]\n",
    "    \n",
    "    # Convert list of indices to tensor and append to s_tensors\n",
    "    s_tensors.append(torch.tensor(sent_ind, dtype=torch.long))\n",
    "    \n",
    "# If '<UNK>' token is not already present in word-to-index dictionary\n",
    "if '<UNK>' not in w2i:\n",
    "    # Add '<UNK>' token to word-to-index dictionary with current index value\n",
    "    w2i.update({'<UNK>':len(w2i)})\n",
    "        \n",
    "# List to store tensor representations of tags\n",
    "t_tensors = []\n",
    "\n",
    "# Iterate over each list of tags in the list of tags\n",
    "for tag_one in t:\n",
    "    # Convert tags to indices using tag-to-index dictionary\n",
    "    tag_ind = [t2i[tag] for tag in tag_one]\n",
    "    t_tensors.append(torch.tensor(tag_ind, dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55573a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLSTM(nn.Module):  # Definition of the BLSTM class as a subclass of the PyTorch nn.Module class.\n",
    "    def __init__(self, vocab_size, tagToIndexD, embedding_dim, hidden_dim, dropout):\n",
    "        super(BLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim  # Set the hidden dimension size\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)  # Define word embedding layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True)  # Define bidirectional LSTM layer\n",
    "        self.hidden1tag = nn.Linear(hidden_dim, hidden_dim // 2)  # Define linear layer for tag prediction\n",
    "        self.hidden2tag = nn.Linear(hidden_dim // 2, len(tagToIndexD))  # Define linear layer for tag prediction\n",
    "        self.dropout = nn.Dropout(p=0.33)  # Define dropout layer with dropout rate of 0.33\n",
    "        self.activation = nn.ELU()  # Define activation function as Exponential Linear Unit (ELU)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)  # Perform word embedding\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))  # Forward pass through LSTM layer\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)  # Reshape LSTM output\n",
    "        lstm_out = self.dropout(lstm_out)  # Apply dropout\n",
    "        tag_space = self.hidden1tag(lstm_out)  # Compute tag scores\n",
    "        tag_scores = self.activation(tag_space)  # Apply activation function\n",
    "        tag_space2 = self.hidden2tag(tag_scores)  # Compute tag scores for the final layer\n",
    "        return tag_space2  # Return the final tag scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b95b4058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "e_dim = 100  # Dimensionality of word embeddings\n",
    "lstm_dropout = 0.33  # Dropout rate for LSTM layer\n",
    "lstm_hidden = 256  # Number of hidden units in LSTM layer\n",
    "lstm_output = 128  # Dimensionality of LSTM output\n",
    "rate_of_l = 0.1  # Learning rate for optimizer\n",
    "epochs = 20  # Number of training epochs\n",
    "\n",
    "# Initialize BLSTM model with defined hyperparameters\n",
    "model = BLSTM(vocab_size=len(w2i),  # Vocabulary size (number of unique words)\n",
    "              embedding_dim=e_dim,  # Dimensionality of word embeddings\n",
    "              dropout=lstm_dropout,  # Dropout rate for LSTM layer\n",
    "              tagToIndexD=t2i,  # Dictionary mapping tags to indices\n",
    "              hidden_dim=lstm_hidden)  # Number of hidden units in LSTM layer\n",
    "\n",
    "# Define loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=rate_of_l)  # Stochastic Gradient Descent (SGD) optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a625df1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Epoch [1/20], Loss: 0.23459\n",
      "Running Epoch [2/20], Loss: 0.28995\n",
      "Running Epoch [3/20], Loss: 0.02823\n",
      "Running Epoch [4/20], Loss: 0.02040\n",
      "Running Epoch [5/20], Loss: 0.00493\n",
      "Running Epoch [6/20], Loss: 0.00825\n",
      "Running Epoch [7/20], Loss: 0.00937\n",
      "Running Epoch [8/20], Loss: 0.00280\n",
      "Running Epoch [9/20], Loss: 0.00127\n",
      "Running Epoch [10/20], Loss: 0.05738\n",
      "Running Epoch [11/20], Loss: 0.00041\n",
      "Running Epoch [12/20], Loss: 0.03127\n",
      "Running Epoch [13/20], Loss: 0.00353\n",
      "Running Epoch [14/20], Loss: 0.26481\n",
      "Running Epoch [15/20], Loss: 0.00133\n",
      "Running Epoch [16/20], Loss: 0.00032\n",
      "Running Epoch [17/20], Loss: 0.00004\n",
      "Running Epoch [18/20], Loss: 0.00005\n",
      "Running Epoch [19/20], Loss: 0.00278\n",
      "Running Epoch [20/20], Loss: 0.00024\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(epochs):\n",
    "    for index in range(len(s_tensors)):\n",
    "        sentence = s_tensors[index]\n",
    "        tags = t_tensors[index]\n",
    "        \n",
    "        # Clear accumulated gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        op_tags = model(sentence)\n",
    "        \n",
    "        # Calculate loss and perform backpropagation\n",
    "        loss = loss_function(op_tags, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # for each epoch, iterating over sentences_tensors, and extracting a single sentence tensors and\n",
    "        # single sentence tag tensors, then clears the gradients of all model parameters before the forward pas\n",
    "        # performs a forward pass through the model to generate predicted tags for the current input sentence.\n",
    "        # calculates the loss between the predicted tags and the true tags for the current input sentence.\n",
    "        # performs backpropagation to compute the gradients of the loss with respect to all model parameters.\n",
    "        \n",
    "        # updates the model parameters based on the gradients computed during backpropagation.\n",
    "    print('Running Epoch [{}/{}], Loss: {:.5f}'.format(epoch+1, epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74e7cc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'blstm1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ea2761",
   "metadata": {},
   "source": [
    "### Task: 1 -- Testing on Dev Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16edf8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_path = \"data/dev\"  # Path to the dev data file\n",
    "dev_lines = []  # List to store lines of dev data\n",
    "s_count = 0  # Counter for sentences\n",
    "\n",
    "# Open the dev data file and read all lines\n",
    "with open(dev_path) as file:\n",
    "    i_lines = file.readlines()\n",
    "\n",
    "    # Iterate over each line in the file\n",
    "    for line in i_lines:\n",
    "        \n",
    "        # Split the line into words\n",
    "        words = line.split(\" \")  \n",
    "        \n",
    "        # List to store current line data\n",
    "        present_l = []  \n",
    "\n",
    "        if len(words) == 1:  # If the line has only one word (indicating end of sentence)\n",
    "            present_l.append(\" \")  # Append an empty string to curline\n",
    "            dev_lines.append(present_l)  # Append curline to dev_lines\n",
    "            continue  # Move to the next iteration\n",
    "\n",
    "        else:  # If the line has more than one word\n",
    "            index = words[0].strip()  # Get the index from the line\n",
    "            present_w = words[1].strip()  # Get the current word from the line\n",
    "            tag = words[2].strip()  # Get the tag from the line\n",
    "\n",
    "            present_l.append(index)  # Append index to curline\n",
    "            present_l.append(present_w)  # Append current word to curline\n",
    "            present_l.append(tag)  # Append tag to curline\n",
    "\n",
    "        dev_lines.append(present_l)  # Append curline to dev_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "522216d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_sentences = []  # List to store sentences in dev data\n",
    "present_s = []  # List to store words of current sentence\n",
    "\n",
    "# Iterate over each line in the dev data\n",
    "for line in dev_lines:\n",
    "    if len(line) == 1:  # If the line contains only one element (empty line indicating end of sentence)\n",
    "        d_sentences.append(present_s)  # Append words of current sentence to dev_sentences\n",
    "        present_s = []  # Reset currS to empty list for the next sentence\n",
    "    else:  # If the line contains more than one element\n",
    "        present_s.append(line[1])  # Append word to currS\n",
    "\n",
    "# Append the last sentence to dev_sentences\n",
    "d_sentences.append(present_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "796a69e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tags = []  # List to store tags for each sentence in dev data\n",
    "present_s = []  # List to store tags of current sentence\n",
    "\n",
    "# Iterate over each line in the dev data\n",
    "for line in dev_lines:\n",
    "    if len(line) == 1:  # If the line contains only one element (empty line indicating end of sentence)\n",
    "        d_tags.append(present_s)  # Append tags of current sentence to dev_tags\n",
    "        present_s = []  # Reset current_sentence to empty list for the next sentence\n",
    "    else:  # If the line contains more than one element\n",
    "        present_s.append(line[2])  # Append tag to current_sentence\n",
    "\n",
    "# Append the last sentence tags to dev_tags\n",
    "d_tags.append(present_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90753e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_sentences_tensors = []  # List to store tensors representing word indices for each sentence in dev data\n",
    "\n",
    "# Iterate over each sentence in the list of dev sentences\n",
    "for s in d_sentences:\n",
    "    sent_index = [w2i.get(word, w2i['<UNK>']) for word in s]  # Convert words to indices using word-to-index dictionary\n",
    "    d_sentences_tensors.append(torch.tensor(sent_index, dtype=torch.long))  # Convert list of indices to PyTorch tensor and append to dev_sentences_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c2f827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient calculation to speed up inference and reduce memory consumption\n",
    "    dev_tag_ops = []  # List to store model outputs for dev data\n",
    "    for s in d_sentences_tensors:  # Iterate over each sentence tensor in dev data\n",
    "        t_op = model(s)  # Pass the sentence tensor through the model to get tag predictions\n",
    "        dev_tag_ops.append(t_op)  # Append model output (tag predictions) to dev_tag_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2dfae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_predicted_tags_ner = []  # List to store predicted tags for each sentence in dev data\n",
    "\n",
    "# Iterate over each set of tag scores (predictions) for dev data\n",
    "for tag_scores in dev_tag_ops:\n",
    "    _, predicted_tags = torch.max(tag_scores, dim=1)  # Get the index of the tag with the highest score for each word\n",
    "    # Convert predicted tag indices to tag labels using the tag-to-index dictionary (t2i)\n",
    "    dev_predicted_tags_ner.append([list(t2i.keys())[i] for i in predicted_tags.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4dbd8efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the list of predicted tags for dev data\n",
    "dev_predicted_tags_write = [tag for sentence_tags in dev_predicted_tags_ner for tag in sentence_tags]\n",
    "\n",
    "# Flatten the list of true tags for dev data\n",
    "dev_tags_flat = [tag for sentence_tags in d_tags for tag in sentence_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15fb2122",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = open(\"dev1.out\", \"w\")  # Open a file named \"task1_dev.out\" for writing\n",
    "write = []  # List to store lines of output for writing to the file\n",
    "\n",
    "# Iterate over each line index in the range of the number of lines in dev data\n",
    "index = 0  # Initialize index for iterating over predicted tags\n",
    "for line in range(len(dev_lines)):\n",
    "    if len(dev_lines[line]) == 1:  # If the line contains only one element (indicating end of sentence)\n",
    "        write.append(\"\\n\")  # Append a blank line to indicate end of sentence in the output file\n",
    "    else:  # If the line contains more than one element\n",
    "        # Construct a line of output containing index, word, actual NER tag, and predicted NER tag\n",
    "        curLine = dev_lines[line][0] + \" \" + dev_lines[line][1] + \" \" + dev_lines[line][2] + \" \" + dev_predicted_tags_write[index] + \"\\n\"\n",
    "        index += 1  # Increment index for the next predicted tag\n",
    "        write.append(curLine)  # Append the constructed line to the list\n",
    "\n",
    "# Write the lines of output to the file\n",
    "out_file.writelines(write)\n",
    "\n",
    "# Close the file\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf3961df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51578 tokens with 5942 phrases; found: 5184 phrases; correct: 4138.\r\n",
      "accuracy:  94.88%; precision:  79.82%; recall:  69.64%; FB1:  74.38\r\n",
      "              LOC: precision:  93.82%; recall:  75.23%; FB1:  83.50  1473\r\n",
      "             MISC: precision:  88.13%; recall:  71.69%; FB1:  79.07  750\r\n",
      "              ORG: precision:  65.20%; recall:  66.37%; FB1:  65.78  1365\r\n",
      "              PER: precision:  75.50%; recall:  65.42%; FB1:  70.10  1596\r\n"
     ]
    }
   ],
   "source": [
    "!python eval.py -p dev1.out -g data/dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7a5714",
   "metadata": {},
   "source": [
    "### Task: 1 -- Testing on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b8f243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"data/test\"  # Path to the test data file\n",
    "test_lines = []  # List to store lines of test data\n",
    "s_count = 0  # Counter for sentences\n",
    "\n",
    "# Open the test data file and read all lines\n",
    "with open(test_path) as file:\n",
    "    t_lines = file.readlines()\n",
    "\n",
    "    # Iterate over each line in the file\n",
    "    for line in t_lines:\n",
    "        words = line.split(\" \")  # Split the line into words\n",
    "        present_l = []  # List to store current line data\n",
    "\n",
    "        if len(words) == 1:  # If the line has only one word (indicating end of sentence)\n",
    "            present_l.append(\" \")  # Append an empty string to present_l\n",
    "            test_lines.append(present_l)  # Append present_l to test_lines\n",
    "            continue  # Move to the next iteration\n",
    "\n",
    "        else:  # If the line has more than one word\n",
    "            index = words[0].strip()  # Get the index from the line\n",
    "            present_w = words[1].strip()  # Get the current word from the line\n",
    "            present_l.append(index)  # Append index to present_l\n",
    "            present_l.append(present_w)  # Append current word to present_l\n",
    "\n",
    "        test_lines.append(present_l)  # Append present_l to test_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "326713af",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sentences = []  # List to store sentences in test data\n",
    "present_s = []  # List to store words of current sentence\n",
    "\n",
    "# Iterate over each line in the test data\n",
    "for line in test_lines:\n",
    "    if len(line) == 1:  # If the line contains only one element (empty line indicating end of sentence)\n",
    "        t_sentences.append(present_s)  # Append words of current sentence to t_sentences\n",
    "        present_s = []  # Reset present_s to empty list for the next sentence\n",
    "    else:  # If the line contains more than one element\n",
    "        present_s.append(line[1])  # Append word to present_s\n",
    "\n",
    "# Append the last sentence to t_sentences\n",
    "t_sentences.append(present_s)\n",
    "\n",
    "t_sentences_tensors = []  # List to store tensors representing word indices for each sentence in test data\n",
    "\n",
    "# Iterate over each sentence in the list of test sentences\n",
    "for s in t_sentences:\n",
    "    s_index = [w2i.get(word, w2i['<UNK>']) for word in s]  # Convert words to indices using word-to-index dictionary\n",
    "    t_sentences_tensors.append(torch.tensor(s_index, dtype=torch.long))  # Convert list of indices to PyTorch tensor and append to t_sentences_tensors\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient calculation to speed up inference and reduce memory consumption\n",
    "    test_tag_ops = []  # List to store model outputs for test data\n",
    "    # Iterate over each sentence tensor in test data\n",
    "    for s in t_sentences_tensors:\n",
    "        t_op = model(s)  # Pass the sentence tensor through the model to get tag predictions\n",
    "        test_tag_ops.append(t_op)  # Append model output (tag predictions) to test_tag_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb0f93d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted_tags_ner = []  # List to store predicted tags for each sentence in test data\n",
    "\n",
    "# Iterate over each set of tag scores (predictions) for test data\n",
    "for t_op in test_tag_ops:\n",
    "    _, predicted_tags = torch.max(t_op, dim=1)  # Get the index of the tag with the highest score for each word\n",
    "    # Convert predicted tag indices to tag labels using the tag-to-index dictionary (t2i)\n",
    "    test_predicted_tags_ner.append([list(t2i.keys())[i] for i in predicted_tags.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87f6d012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the test_predicted_tags_ner is flatted such that it can be iterated and written back to the test.out\n",
    "test_predicted_tags_flat = [tag for test_sentence_tags in test_predicted_tags_ner for tag in test_sentence_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44e5d6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_out_file = open(\"test1.out\", \"w\")  # Open a file named \"task1_test.out\" for writing\n",
    "write = []  # List to store lines of output for writing to the file\n",
    "index = 0  # Initialize index for iterating over predicted tags\n",
    "\n",
    "# Iterate over each line index in the range of the number of lines in test data\n",
    "for line in range(len(test_lines)):\n",
    "    if len(test_lines[line]) == 1:  # If the line contains only one element (indicating end of sentence)\n",
    "        write.append(\"\\n\")  # Append a blank line to indicate end of sentence in the output file\n",
    "    else:  # If the line contains more than one element\n",
    "        # Construct a line of output containing index, word, and predicted NER tag\n",
    "        present_l = test_lines[line][0] + \" \" + test_lines[line][1] + \" \" + test_predicted_tags_flat[index] + \"\\n\"\n",
    "        index += 1  # Increment index for the next predicted tag\n",
    "        write.append(present_l)  # Append the constructed line to the list\n",
    "\n",
    "# Write the lines of output to the file\n",
    "test_out_file.writelines(write)\n",
    "\n",
    "# Close the file\n",
    "test_out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8b6037",
   "metadata": {},
   "source": [
    "### Task:2 - Using Glove Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f467d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_E = {}  # Dictionary to store GloVe word embeddings\n",
    "\n",
    "# Open the GloVe file and read each line\n",
    "with open('glove.6B.100d', 'r') as g_file:\n",
    "    # Iterate over each line in the file\n",
    "    for line in g_file:\n",
    "        index = line.split()  # Split the line into elements\n",
    "        present_w = index[0]  # Extract the word from the line\n",
    "        word_v = np.array(index[1:], dtype=np.float32)  # Extract the word vector and convert it to a numpy array\n",
    "        g_E[present_w] = word_v  # Add the word and its corresponding vector to the GloVe embeddings dictionary\n",
    "\n",
    "# Check if '<PAD>' is not in the word-to-index dictionary (w2i)\n",
    "if '<PAD>' not in w2i:\n",
    "    w2i.update({'<PAD>': len(w2i)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49687358",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloveBLSTM(nn.Module): # This defines a new PyTorch module called GloveBLSTM.\n",
    "\n",
    "    def __init__(self, vocab_size, tag_size, embedding_dim, hidden_dim, dropout, word_to_embedding):\n",
    "        super(GloveBLSTM, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # This is an embedding layer that maps input words to their GloVe embeddings.\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_to_embedding = word_to_embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim//2, num_layers=1, bidirectional=True, dropout=dropout)\n",
    "        \n",
    "        # This is a bidirectional LSTM layer that processes the input embeddings\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, hidden_dim//2)\n",
    "        \n",
    "        #This is a linear layer that maps the LSTM output to a smaller dimensionality.\n",
    "        self.dropout = nn.Dropout(p=0.33)\n",
    "        \n",
    "        #This is a dropout layer that randomly sets a fraction of the inputs to zero during training, to pr\n",
    "        self.activation = nn.ELU()\n",
    "        \n",
    "        #This applies an activation function to the output of the self.hidden2tag layer.\n",
    "        self.hidden3tag = nn.Linear(hidden_dim//2, tag_size)\n",
    "                            \n",
    "    def forward(self, sentence):\n",
    "        embeddings = []\n",
    "        \n",
    "        # If a word is not in the pre-trained embeddings,the code generates a random vector using NumPy's np.random.normal() function with a scale of 0.6\n",
    "        # and the same dimensionality as the pre-trained embeddings.This is done in the if clause of the forward method where it checks if the current word is in the\n",
    "        for word in sentence:\n",
    "            if word.lower() in self.word_to_embedding:\n",
    "                embeddings.append(self.word_to_embedding[word.lower()])\n",
    "            else:\n",
    "                embeddings.append(np.random.normal(scale=0.6, size=self.embedding_dim))\n",
    "        embeddings = torch.tensor(embeddings)\n",
    "        embeddings = embeddings.type(torch.float32)\n",
    "        embeddings = embeddings.unsqueeze(0)\n",
    "        lstm_out, _= self.lstm(embeddings)\n",
    "        lstm_out = lstm_out.squeeze(0)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        tag_scores = self.activation(tag_space)\n",
    "        tag_scores_final = self.hidden3tag(tag_scores)\n",
    "        return tag_scores_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14be6eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters for the GloveBLSTM model\n",
    "v_c = len(w2i)  # Vocabulary size (number of unique words)\n",
    "t_c = len(t2i)  # Number of unique NER tags\n",
    "h_d = 256  # Dimensionality of LSTM hidden states\n",
    "output_dimensions = 128  # Dimensionality of linear layer output\n",
    "e_dim = 100  # Dimensionality of word embeddings\n",
    "d_o = 0.33  # Dropout rate\n",
    "\n",
    "# Create an instance of GloveBLSTM model with the specified hyperparameters\n",
    "g_model = GloveBLSTM(v_c, t_c, e_dim, h_d, d_o, g_E)\n",
    "\n",
    "# Define the loss function as CrossEntropyLoss\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer as SGD (Stochastic Gradient Descent) with the specified learning rate\n",
    "optimizer = optim.SGD(g_model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff463b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Epoch [1/20] and the loss is: 0.3669600\n",
      "Running Epoch [2/20] and the loss is: 0.2911047\n",
      "Running Epoch [3/20] and the loss is: 0.2693533\n",
      "Running Epoch [4/20] and the loss is: 0.2553874\n",
      "Running Epoch [5/20] and the loss is: 0.2436230\n",
      "Running Epoch [6/20] and the loss is: 0.2357930\n",
      "Running Epoch [7/20] and the loss is: 0.2274897\n",
      "Running Epoch [8/20] and the loss is: 0.2237183\n",
      "Running Epoch [9/20] and the loss is: 0.2201400\n",
      "Running Epoch [10/20] and the loss is: 0.2163373\n",
      "Running Epoch [11/20] and the loss is: 0.2102483\n",
      "Running Epoch [12/20] and the loss is: 0.2087261\n",
      "Running Epoch [13/20] and the loss is: 0.2056789\n",
      "Running Epoch [14/20] and the loss is: 0.2009405\n",
      "Running Epoch [15/20] and the loss is: 0.2015911\n",
      "Running Epoch [16/20] and the loss is: 0.1988005\n",
      "Running Epoch [17/20] and the loss is: 0.1965594\n",
      "Running Epoch [18/20] and the loss is: 0.1952963\n",
      "Running Epoch [19/20] and the loss is: 0.1936225\n",
      "Running Epoch [20/20] and the loss is: 0.1922456\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each epoch\n",
    "for epoch in range(epochs):\n",
    "    t_l = 0  # Initialize total loss for the epoch\n",
    "    # Iterate over each sentence and its corresponding tags\n",
    "    for i in range(len(sents)):\n",
    "        s = sents[i]  # Get the i-th sentence\n",
    "        tags = t_tensors[i]  # Get the tags for the i-th sentence\n",
    "        g_model.zero_grad()  # Clear gradients from previous iteration\n",
    "        results = g_model(s)  # Get the model predictions for the sentence\n",
    "        loss = loss_function(results, tags)  # Compute the loss\n",
    "        loss.backward()  # Backpropagate the loss\n",
    "        optimizer.step()  # Update model parameters using optimizer\n",
    "        t_l += loss.item()  # Accumulate the loss for the epoch\n",
    "    # Print the average loss for the epoch\n",
    "    print(f\"Running Epoch [{epoch+1}/{epochs}] and the loss is: {t_l/len(sents):.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c889ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(g_model, 'blstm2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e070f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_model = torch.load('blstm2.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9672a630",
   "metadata": {},
   "source": [
    "### Task:2 -- Testing on Dev Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0114ff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_sentences = []  # List to store sentences in dev data\n",
    "present_s = []  # List to store words of current sentence\n",
    "\n",
    "# Iterate over each line in the dev data\n",
    "for line in dev_lines:\n",
    "    if len(line) == 1:  # If the line contains only one element (empty line indicating end of sentence)\n",
    "        d_sentences.append(present_s)  # Append words of current sentence to dev_sentences\n",
    "        present_s = []  # Reset currS to empty list for the next sentence\n",
    "    else:  # If the line contains more than one element\n",
    "        present_s.append(line[1])  # Append word to currS\n",
    "\n",
    "# Append the last sentence to dev_sentences\n",
    "d_sentences.append(present_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9e619a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tags = []  # List to store tags for each sentence in dev data\n",
    "present_s = []  # List to store tags of current sentence\n",
    "\n",
    "# Iterate over each line in the dev data\n",
    "for line in dev_lines:\n",
    "    if len(line) == 1:  # If the line contains only one element (empty line indicating end of sentence)\n",
    "        d_tags.append(present_s)  # Append tags of current sentence to dev_tags\n",
    "        present_s = []  # Reset current_sentence to empty list for the next sentence\n",
    "    else:  # If the line contains more than one element\n",
    "        present_s.append(line[2])  # Append tag to current_sentence\n",
    "\n",
    "# Append the last sentence tags to dev_tags\n",
    "d_tags.append(present_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3586150b",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient calculation to speed up inference and reduce memory consumption\n",
    "    dev_tag_scores = []  # List to store model outputs for dev data\n",
    "    for s in d_sentences:  # Iterate over each sentence tensor in dev data\n",
    "        t_op = g_model(s)  # Pass the sentence tensor through the model to get tag predictions\n",
    "        dev_tag_scores.append(t_op)  # Append model output (tag predictions) to dev_tag_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05ce9497",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_predicted_tags_ner = []  # List to store predicted tags for each sentence in dev data\n",
    "\n",
    "# Iterate over each set of tag scores (predictions) for dev data\n",
    "for tag_scores in dev_tag_scores:\n",
    "    _, predicted_tags = torch.max(tag_scores, dim=1)  # Get the index of the tag with the highest score for each word\n",
    "    # Convert predicted tag indices to tag labels using the tag-to-index dictionary (t2i)\n",
    "    dev_predicted_tags_ner.append([list(t2i.keys())[i] for i in predicted_tags.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c1645035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the list of predicted tags for dev data\n",
    "dev_predicted_tags_write = [tag for sentence_tags in dev_predicted_tags_ner for tag in sentence_tags]\n",
    "\n",
    "# Flatten the list of true tags for dev data\n",
    "dev_tags_flat = [tag for sentence_tags in d_tags for tag in sentence_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9cec5afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = open(\"dev2.out\", \"w\")  # Open a file named \"task1_dev.out\" for writing\n",
    "write = []  # List to store lines of output for writing to the file\n",
    "\n",
    "# Iterate over each line index in the range of the number of lines in dev data\n",
    "index = 0  # Initialize index for iterating over predicted tags\n",
    "for line in dev_lines:\n",
    "    if len(line) == 1:  # If the line contains only one element (indicating end of sentence)\n",
    "        write.append(\"\\n\")  # Append a blank line to indicate end of sentence in the output file\n",
    "    else:  # If the line contains more than one element\n",
    "        # Construct a line of output containing index, word, actual NER tag, and predicted NER tag\n",
    "        curLine = line[0] + \" \" + line[1] + \" \" + line[2] + \" \" + dev_predicted_tags_write[index] + \"\\n\"\n",
    "        index += 1  # Increment index for the next predicted tag\n",
    "        write.append(curLine)  # Append the constructed line to the list\n",
    "\n",
    "# Write the lines of output to the file\n",
    "out_file.writelines(write)\n",
    "\n",
    "# Close the file\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ea7f37a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51578 tokens with 5942 phrases; found: 5815 phrases; correct: 5134.\r\n",
      "accuracy:  97.34%; precision:  88.29%; recall:  86.40%; FB1:  87.34\r\n",
      "              LOC: precision:  91.46%; recall:  91.56%; FB1:  91.51  1839\r\n",
      "             MISC: precision:  84.16%; recall:  75.49%; FB1:  79.59  827\r\n",
      "              ORG: precision:  81.14%; recall:  76.66%; FB1:  78.83  1267\r\n",
      "              PER: precision:  91.82%; recall:  93.81%; FB1:  92.80  1882\r\n"
     ]
    }
   ],
   "source": [
    "!python eval.py -p dev2.out -g data/dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca555f4f",
   "metadata": {},
   "source": [
    "### Task:2 -- Testing on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea35feb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sentences = []  # List to store sentences in test data\n",
    "present_s = []  # List to store words of current sentence\n",
    "\n",
    "# Iterate over each line in the test data\n",
    "for line in test_lines:\n",
    "    if len(line) == 1:  # If the line contains only one element (empty line indicating end of sentence)\n",
    "        t_sentences.append(present_s)  # Append words of current sentence to t_sentences\n",
    "        present_s = []  # Reset present_s to empty list for the next sentence\n",
    "    else:  # If the line contains more than one element\n",
    "        present_s.append(line[1])  # Append word to present_s\n",
    "\n",
    "# Append the last sentence to t_sentences\n",
    "t_sentences.append(present_s)\n",
    "\n",
    "g_model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient calculation to speed up inference and reduce memory consumption\n",
    "    test_tag_ops = []  # List to store model outputs for test data\n",
    "    # Iterate over each sentence tensor in test data\n",
    "    for s in t_sentences:\n",
    "        t_op = g_model(s)  # Pass the sentence tensor through the model to get tag predictions\n",
    "        test_tag_ops.append(t_op)  # Append model output (tag predictions) to test_tag_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6fecbbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted_tags_ner = []  # List to store predicted tags for each sentence in test data\n",
    "\n",
    "# Iterate over each set of tag scores (predictions) for test data\n",
    "for t_op in test_tag_ops:\n",
    "    _, predicted_tags = torch.max(t_op, dim=1)  # Get the index of the tag with the highest score for each word\n",
    "    # Convert predicted tag indices to tag labels using the tag-to-index dictionary (t2i)\n",
    "    test_predicted_tags_ner.append([list(t2i.keys())[i] for i in predicted_tags.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e78399b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the test_predicted_tags_ner is flatted such that it can be iterated and written back to the test.out\n",
    "test_predicted_tags_flat = [tag for test_sentence_tags in test_predicted_tags_ner for tag in test_sentence_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0dfe06fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_out_file = open(\"test2.out\", \"w\")  # Open a file named \"task1_test.out\" for writing\n",
    "write = []  # List to store lines of output for writing to the file\n",
    "index = 0  # Initialize index for iterating over predicted tags\n",
    "\n",
    "# Iterate over each line index in the range of the number of lines in test data\n",
    "for line in range(len(test_lines)):\n",
    "    if len(test_lines[line]) == 1:  # If the line contains only one element (indicating end of sentence)\n",
    "        write.append(\"\\n\")  # Append a blank line to indicate end of sentence in the output file\n",
    "    else:  # If the line contains more than one element\n",
    "        # Construct a line of output containing index, word, and predicted NER tag\n",
    "        present_l = test_lines[line][0] + \" \" + test_lines[line][1] + \" \" + test_predicted_tags_flat[index] + \"\\n\"\n",
    "        index += 1  # Increment index for the next predicted tag\n",
    "        write.append(present_l)  # Append the constructed line to the list\n",
    "\n",
    "# Write the lines of output to the file\n",
    "test_out_file.writelines(write)\n",
    "\n",
    "# Close the file\n",
    "test_out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e16927f",
   "metadata": {},
   "source": [
    "### Task 2 Complete"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
